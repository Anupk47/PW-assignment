{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main difference between the Euclidean and Manhattan distance metrics is in how they measure the distance between two points:\n",
    "\n",
    "# Euclidean Distance: It is the straight-line (or \"as-the-crow-flies\") distance between two points, calculated using the Pythagorean theorem. \n",
    "# The formula for Euclidean distance between two points \n",
    "#d = root ((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "# Manhattan Distance: It is the sum of the absolute differences between the corresponding coordinates of two points. \n",
    "# It is like calculating the distance in a grid, moving along the grid lines. The formula for Manhattan distance is:\n",
    "#formula : d = |x1 - x2| + |y1 - y2|\n",
    "\n",
    "# Impact on Performance:\n",
    "\n",
    "# Euclidean Distance tends to be more sensitive to large differences in the values of the features and is ideal when the data points are close to each other in a multi-dimensional space.\n",
    "\n",
    "# Manhattan Distance is better when features are independent and the data has a grid-like structure or the features are measured in different units (e.g., categorical features that can take integer values).\n",
    "\n",
    "# In terms of performance:\n",
    "\n",
    "# KNN Classifier/Regressor: The choice of distance metric can affect the accuracy and prediction quality. Euclidean distance is sensitive to outliers and can be biased if the feature scales are not uniform. \n",
    "# Manhattan distance, however, is more robust in high-dimensional or grid-like data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the optimal value of (the number of neighbors) is critical for the performance of the KNN algorithm. If is too small, the model may overfit (high variance), and if is too large, the model may underfit (high bias).\n",
    "\n",
    "# Techniques for choosing optimal  Cross-validation: Split the dataset into training and validation sets, and test different values of  on the validation set. The that provides the best cross-validation score (accuracy, F1-score, etc.) is usually selected.\n",
    "\n",
    "# Elbow Method (for classification): Plot the error rate against different values of  The optimal value of  typically corresponds to the \"elbow\" of the graph, where the error rate stops decreasing significantly.\n",
    "\n",
    "# Grid Search: For more rigorous searching, use grid search with cross-validation, which evaluates all possible  values and selects the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The choice of the distance metric can significantly affect the performance of a KNN classifier or regressor, as it directly influences how neighbors are identified:\n",
    "# Euclidean Distance: This metric works well when features are continuous and the data points are distributed relatively uniformly in space. It's ideal when the relationship between features is more linear and the features have similar scales.\n",
    "# Manhattan Distance: This metric is better suited for situations where the features are not uniformly distributed, or the data points exist in a grid-like or discrete structure. It is less sensitive to outliers and can be more appropriate when dealing with categorical or ordinal data.\n",
    "\n",
    "# Situations for choosing each:\n",
    "# Euclidean Distance: Use when features are continuous and relatively homogeneous, such as in image recognition or sensor data.\n",
    "# Manhattan Distance: Use when data is sparse, features are categorical, or there is a grid-like structure (e.g., geographic data, or urban grid dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyperparameters in KNN are:\n",
    "# K (Number of Neighbors): This is the number of nearest neighbors to consider when making predictions. A small value of k can lead to overfitting, while a large value can lead to underfitting. The choice of k is critical for model performance.\n",
    "\n",
    "# Distance Metric: As discussed, the choice of distance metric (Euclidean, Manhattan, etc.) affects how neighbors are identified and can influence performance, especially in high-dimensional spaces.\n",
    "# Weights (Uniform/Distance): KNN allows weighting the neighbors' contribution to the prediction.\n",
    "# Uniform Weights: All neighbors are treated equally.\n",
    "# Distance Weights: Neighbors closer to the query point are given more weight.\n",
    "# Algorithm: KNN can use different algorithms for nearest neighbor search, such as brute force, KD-Tree, or Ball-Tree. The choice of algorithm can affect computation speed, especially for large datasets.\n",
    "# Tuning Hyperparameters:\n",
    "\n",
    "# Grid Search with Cross-validation: Use this method to find the optimal combination of k, distance metric, and weights that give the best model performance.\n",
    "# Randomized Search: For large search spaces, randomized search can be more efficient than grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the training set plays a significant role in KNN performance:\n",
    "# Smaller training sets: May cause KNN to overfit, as there are fewer examples to generalize from. The model may not capture the underlying patterns well and could perform poorly on new data.\n",
    "# Larger training sets: Offer better generalization, but the computation time to find nearest neighbors increases as the training set size grows, leading to slower predictions.\n",
    "\n",
    "# Optimizing training set size:\n",
    "# Subsampling: If the dataset is very large, use a subset of the data (without losing diversity) to improve speed.\n",
    "# Dimensionality Reduction (PCA, t-SNE): Reduce the number of features while preserving the variability in the data to speed up the process and improve performance.\n",
    "# Data Augmentation: For classification, especially with imbalanced datasets, consider data augmentation techniques like SMOTE or random oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawbacks of KNN:\n",
    "\n",
    "# Computational Complexity: As the dataset grows, the computational cost of calculating distances between all data points becomes prohibitive, especially during prediction.\n",
    "# Solution: Use efficient data structures like KD-trees or Ball-trees for faster nearest-neighbor search.\n",
    "\n",
    "# Sensitive to Irrelevant Features: KNN can be affected by irrelevant or redundant features that donâ€™t contribute to the target prediction.\n",
    "# Solution: Perform feature selection or dimensionality reduction (PCA, LDA) to reduce the effect of irrelevant features.\n",
    "\n",
    "# High Memory Usage: KNN stores the entire training dataset, which can be memory-intensive, especially for large datasets.\n",
    "# Solution: Use techniques like approximate nearest neighbors to reduce memory usage while maintaining accuracy.\n",
    "\n",
    "# Curse of Dimensionality: KNN suffers as the number of features increases because the \"distance\" between points becomes less meaningful.\n",
    "# Solution: Use feature selection or dimensionality reduction to reduce the number of features, improving the model's performance.\n",
    "\n",
    "# Outliers: KNN is sensitive to outliers, which can mislead the distance calculations.\n",
    "# Solution: Use robust distance metrics (like Manhattan distance) or pre-process the data by removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
