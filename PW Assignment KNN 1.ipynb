{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KNN (K-Nearest Neighbors) algorithm is a supervised machine learning algorithm used for both classification and regression tasks.\n",
    "# It works by identifying the 'K' closest data points to a given query point, based on a distance metric (typically Euclidean distance).\n",
    "# For classification, the algorithm assigns the class most common among the K neighbors, and for regression, it computes the average (or weighted average) of the K nearest neighbors' target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value of K is a crucial parameter in the KNN algorithm. A small value of K can lead to a noisy model that overfits the data, while a large value of K can cause underfitting, where the model becomes too simple and misses patterns. Typically, you choose K using methods like:\n",
    "\n",
    "# Cross-validation: Evaluate the model performance with different K values and choose the one that gives the best result.\n",
    "\n",
    "# Odd values of K: Using an odd number of K can prevent ties in classification tasks.\n",
    "\n",
    "# Domain knowledge: Sometimes, prior knowledge about the data distribution helps in choosing an optimal K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier: Used for classification tasks, where the output is a category or class label. It assigns a class based on the majority class among the K nearest neighbors.\n",
    "\n",
    "# KNN Regressor: Used for regression tasks, where the output is a continuous value. It calculates the average of the K nearest neighbors' values as the predicted outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance of the KNN algorithm can be measured using several metrics depending on the task:\n",
    "\n",
    "# For classification: Common metrics include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "# For regression: Metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared are used to assess the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The curse of dimensionality refers to the issue that arises when the number of features (dimensions) in the dataset increases. \n",
    "# As the number of dimensions grows, the data points become sparse, and the concept of \"nearness\" becomes less meaningful. In KNN, this results in the algorithm being less effective, as the distances between points become more similar and the model's ability to distinguish between neighbors diminishes. \n",
    "# This issue can lead to decreased accuracy and slower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values in KNN typically involves:\n",
    "\n",
    "# Imputation: Missing values can be imputed with the mean, median, or mode of the respective feature column or using advanced techniques like KNN imputation.\n",
    "\n",
    "# Ignoring missing data: If the amount of missing data is small, rows with missing values can be removed.\n",
    "\n",
    "# Using KNN imputation: You can use KNN to predict missing values based on the nearest neighbors' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier: It is well-suited for classification problems where the output is categorical. It works well when classes are clearly separable but can be sensitive to noisy or overlapping classes. \n",
    "# It's better when the decision boundaries are complex and non-linear.\n",
    "\n",
    "# KNN Regressor: It is best for regression tasks, where the output is a continuous value. It works well when the relationship between the features and the target variable is non-linear and doesn't follow a clear linear trend.\n",
    "\n",
    "# In general, KNN Classifier is better for classification tasks, and KNN Regressor is better for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strengths:\n",
    "# Simplicity and Intuition: KNN is easy to understand and implement.\n",
    "# No Assumptions About Data: It doesn’t assume any underlying data distribution, making it flexible.\n",
    "# Effective for Non-linear Problems: It works well for non-linear decision boundaries in both classification and regression tasks.\n",
    "\n",
    "# Weaknesses:\n",
    "# High Computational Complexity: KNN can be slow, especially with large datasets, since it computes distances for every test instance.\n",
    "# Curse of Dimensionality: Performance degrades as the number of features increases.\n",
    "# Sensitive to Noisy Data: Outliers and noisy data can have a significant impact on KNN’s performance.\n",
    "\n",
    "# Addressing Weaknesses:\n",
    "# Use dimensionality reduction techniques like PCA to reduce the impact of high-dimensional data.\n",
    "# Use efficient data structures like KD-trees or ball trees to speed up nearest neighbor search.\n",
    "# Apply feature selection to remove irrelevant features and reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance: It is the straight-line distance between two points in a Euclidean space. \n",
    "# It is calculated using the Pythagorean theorem, i.e., the square root of the sum of squared differences between corresponding points:\n",
    "#formula => d = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "# Manhattan Distance: It is the sum of the absolute differences between corresponding points. This distance is also called \"taxicab\" or \"city block\" distance:\n",
    "#formula => d = |x2 - x1| + |y2 - y1|\n",
    "#In KNN, Euclidean distance is typically used for continuous variables, while Manhattan distance is used when features are more discrete or on a grid-like structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling is critical in KNN because the algorithm relies on distance calculations between data points. If the features have different scales (e.g., one feature ranges from 1 to 10, while another ranges from 1000 to 10000), the larger scaled feature will dominate the distance calculation, potentially leading to biased results. Scaling ensures that each feature contributes equally to the distance metric. Common methods for feature scaling include:\n",
    "\n",
    "# Min-Max Scaling\n",
    "\n",
    "# Standardization (Z-score normalization)\n",
    "\n",
    "# Scaling helps to avoid this issue and improves the performance and accuracy of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
